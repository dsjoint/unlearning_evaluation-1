defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

just_eval: false
eval_model_paths: [""]
only_ft: false
ft_model_paths: [["Qwen/Qwen2.5-3B-Instruct", "YEARS"]]
dont_ft: false
testing: false
raise_exceptions: false

# Attention backend: auto, flash_attention_2, sdpa, or eager
# - auto (default): Use flash_attention_2 if available, otherwise sdpa
# - flash_attention_2: Use flash attention (requires flash-attn package)
# - sdpa: Use PyTorch's scaled dot-product attention
# - eager: Use standard PyTorch attention (slowest, most compatible)
attn_backend: auto

num_gpus: 1
# model_id: "models/fted/Meta-Llama-3-8B/LossType.LETTER_ANSWER/all_splits/lr1e-06-epoch6"
#model_id: "meta-llama/Meta-Llama-3-8B"
# model_id: "models/random_bd/lr2e-07-epoch15"
model_id: "Qwen/Qwen2.5-3B-Instruct"
num_layers: ${get_num_layers:${model_id}}
datasets: [YEARS]
wandb_project_name: "lora_years_qwen_3b"
results_dir: "evals/pipeline"
data_root: "data"

# Run name for isolating different pipeline runs in models/ directory
# - null (default): Auto-generate timestamp-based name (YYYY-MM-DD_HH-MM-SS)
# - string: Use specified name (required for RTT-only runs with only_ft=true)
# This creates a top-level folder: models/{run_name}/...

#run_name: null
run_name: "2025-12-28_05-13-18"
batch_size: 8
val_batch_size: 16
warmup_steps: 24
data_seed: 4
eval_every: 2

# Accuracy selection rule for A/B/C summary stats
# - final_epoch: Use accuracy at the last epoch
# - max_epoch: Use maximum accuracy across all epochs
acc_selection_rule: final_epoch

# When true, the driver collects per-run A/B/C forget+retain metrics and writes a
# lightweight JSON file to `${results_dir}/summary/raw_results_${run_name}.json`.
# This is useful for seed sweeps when you don't want to save large checkpoints.
write_raw_results: false

# Baseline model validation - skip experiments if base model doesn't know the info
# Set to 0 to disable the check
baseline_min_forget_acc: 0.3

unlearn:
  types: [LORA]
  many_cut_sc: true
  cut_scs: [0.1, 1, 10]
  lora_ranks: [2, 4]
  use_4bit: false  # Disabled for faster training (LoRA instead of QLoRA)
  bnb_4bit_quant_type: nf4
  bnb_4bit_compute_dtype: bf16
  bnb_4bit_double_quant: true
  max_seq_len: 512
  grad_accum_steps: 4
  gradient_checkpointing: true
  save_unlearn_model: true
  # A list of lists; resolves to coefficient * num_layers. ex: [[0, "0.5"]]
  freeze_layers_coeffs: null
  freeze_layers: ${resolve_freeze_layers:${unlearn.freeze_layers_coeffs}, ${model_id}}
  types_config:
    CUT:
      loss_type: CORPUS
      datasets_config:
        YEARS:
          epochs_lst: [3]
          lrs: [1e-6]
          rcs:
            range: ${get_log_range:1e2, 2e4, 10}
            add: [0, 1200] # Additional hand-picked retain_coeffs
        YEARS_MMLU_RETAIN:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e2, 2e4, 10}
            add: [0, 1200] # Additional hand-picked retain_coeffs
        MMLU:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e2, 2e4, 10}
            add: [0, 1200] 
            # add: [1, 2, 4, 8] 
        WMDP_CORPUS:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            # range: ${get_log_range:1e-2, 2e4, 10}
            range: ${get_log_range:1e-2, 1e5, 10}
            add: [0] 
        WMDP_CORPUS_FINEWEB:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            # range: ${get_log_range:1e-2, 2e4, 10}
            range: ${get_log_range:1e-2, 1e5, 10}
            add: [] 
        WMDP_MCQ_CORPUS:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            # range: ${get_log_range:1e-2, 2e4, 10}
            range: ${get_log_range:1e2, 1e4, 2}
            add: [] 
        WMDP_MCQ_CORPUS_FINEWEB:
          epochs_lst: [1, 2]
          lrs: [2e-5, 4e-5, 8e-5]
          rcs:
            # range: ${get_log_range:1e-2, 2e4, 10}
            range: ${get_log_range:1e2, 1e4, 2}
            add: [] 
        WMDP_MCQ_FINEWEB:
          epochs_lst: [1]
          lrs: [4e-5]
          rcs:
            range: ${get_log_range:1e2, 1e4, 2}
            add: [] 
        RANDOM_BD:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-2, 2e4, 10}
            add: [0, 1200] 
    GD:
      loss_type: CORPUS
      datasets_config:
        YEARS:
          epochs_lst: [5]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 0.002]
        YEARS_MMLU_RETAIN:
          epochs_lst: [5]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 0.002]
        MMLU:
          epochs_lst: [5]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP:
          epochs_lst: [5]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP_CORPUS:
          epochs_lst: [2]
          lrs: [4e-7, 8e-7, 16e-7, 32e-7]
          #lrs: [16e-7]
          rcs:
            range: ${get_log_range:1e-2, 1e5, 10}
            add: [0]
        WMDP_CORPUS_FINEWEB:
          epochs_lst: [2]
          lrs: [32e-7]
          rcs:
            # range: ${get_log_range:1e-2, 2e4, 10}
            range: ${get_log_range:1e-2, 1e5, 10}
            #range: ${get_log_range:1e-2, 1e-2, 10}
            add: [] 
        WMDP_CORPUS_MMLU:
          epochs_lst: [8]
          lrs: [16e-7, 32e-7]
          rcs:
            range: ${get_log_range:1e-2, 2e4, 10}
            #range: ${get_log_range:1e1, 1e2, 2}
            #range: ${get_log_range:1e-2, 1e-2, 10}
            add: [0] 
        WMDP_MCQ_CORPUS:
          epochs_lst: [2, 4, 8]
          lrs: [5e-8, 1e-7, 2e-7, 4e-7, 8e-7, 16e-7, 32e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP_MCQ_CORPUS_FINEWEB:
          epochs_lst: [2]
          lrs: [16e-7]
          rcs:
            #range: ${get_log_range:1, 1, 2}
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [2, 8]
        WMDP_MCQ_FINEWEB:
          epochs_lst: [4]
          lrs: [16e-7]
          rcs:
            range: ${get_log_range:1e-1, 1e4, 2}
            add: [0]
        RANDOM_BD:
          epochs_lst: [5]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
    WHP:
      loss_type: CORPUS
      datasets_config:
        YEARS:
          epochs_lst: [5]
          lrs: [8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        YEARS_MMLU_RETAIN:
          epochs_lst: [5]
          lrs: [8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        MMLU:
          epochs_lst: [5]
          lrs: [4e-7, 8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP_MCQ_CORPUS:
          epochs_lst: [5]
          lrs: [4e-7, 8e-7, 16e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP_MCQ_CORPUS_FINEWEB:
          epochs_lst: [5]
          lrs: [32e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        RANDOM_BD:
          epochs_lst: [5]
          lrs: [4e-7, 8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
    FWF:
      loss_type: CORPUS
      datasets_config:
        YEARS:
          epochs_lst: [5]
          lrs: [8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        YEARS_MMLU_RETAIN:
          epochs_lst: [5]
          lrs: [8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        MMLU:
          epochs_lst: [5]
          lrs: [4e-7, 8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP_MCQ_CORPUS:
          epochs_lst: [5]
          lrs: [4e-7, 8e-7, 16e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        RANDOM_BD:
          epochs_lst: [5]
          lrs: [4e-7, 8e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
    LORA:
      loss_type: CORPUS
      layer_selection_mode: "none"  # Options: "none", "learned_topk_hard"
      lora_layer_budget_k: null  # int, required if layer_selection_mode="learned_topk_hard"
      gate_tau_start: 10.0  # Initial temperature
      gate_tau_end: 0.1  # Final temperature
      gate_warmup_steps: 0  # Optional: warmup steps before annealing (default: 0)
      gate_seed: null  # Optional: random seed for gate initialization (default: None, use data_seed)
      gate_reg_coeff: 0.0  # Optional: L2 regularization on gate logits (default: 0.0)
      datasets_config:
        YEARS:
          epochs_lst: [6]
          lrs: [4e-7]
          rcs:
            range: []
            add: [0.001]
        YEARS_MMLU_RETAIN:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        MMLU:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP_MCQ_CORPUS:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        WMDP_MCQ_CORPUS_FINEWEB:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]
        RANDOM_BD:
          epochs_lst: [3]
          lrs: [4e-7]
          rcs:
            range: ${get_log_range:1e-3, 1e3, 10}
            add: [0, 2, 4]

matched_forgetting:
  enabled: false
  target_forget_acc: 0.60
  tolerance: 0.02  # Â±0.02 around target
  max_trials_per_rank: 18
  search_space:
    rc_range: ${get_log_range:0.001, 10.0, 3}  # [0.001, 0.01, 0.1, 1.0, 10.0]
    rc_add: [0.01, 0.1, 1.0]  # Additional hand-picked values
    lr_range: [2e-7, 4e-7, 8e-7]  # Can override per-dataset
    epochs_range: [3, 5, 6]  # Can override per-dataset
  selection_priority: ["retain_damage", "compute", "retain_coeff"]  # Tie-breaking order
  acc_selection_rule: final_epoch  # "final_epoch" or "max_epoch"
  save_all_candidates: true  # Save all candidates during search (Option 1)

ft:
  num_splits: 1
  eval_split_ids: null
  eval_seed: 0
  loss_types: [QUESTION_LETTER_ANSWER]
  # A list of lists; resolves to coefficient * num_layers. ex: [[0, "0.5"]]
  freeze_layers_coeffs: null
  freeze_layers: ${resolve_freeze_layers:${ft.freeze_layers_coeffs}, ${model_id}}
  epochs_lst: [2]
  lrs: [1e-6]
  save_models: true # save models for later use
  # Separate batch sizes for RTT to avoid OOM when running in parallel

hydra:
  run:
    dir: .
