defaults:
  - default
  - _self_

# LoRA rank sweep experiment
just_eval: false
dont_ft: false
testing: false

model_id: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
datasets: [MMLU]
wandb_project_name: "lora_rank_sweep"

unlearn:
  types: [LORA]
  lora_ranks: [1, 2, 4, 8, 16, 32]
  save_unlearn_model: true

ft:
  num_splits: 2
  loss_types: [QUESTION_LETTER_ANSWER]
  epochs_lst: [6]
  lrs: ${get_log_range:1e-7,5e-6,2}
  save_models: false
